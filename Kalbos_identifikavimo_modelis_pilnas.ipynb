{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffe0c91",
   "metadata": {},
   "source": [
    "# 1. Duomenų padalijimas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be82e7e",
   "metadata": {},
   "source": [
    "### 1.1. Trumpi sakiniai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a5a5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiekvienos kalbos sakinių kiekis:\n",
      "deu: 740630\n",
      "eng: 1997242\n",
      "est: 6157\n",
      "fra: 694837\n",
      "ita: 948238\n",
      "lit: 140835\n",
      "lvs: 13442\n",
      "pol: 133982\n",
      "rus: 1165913\n",
      "spa: 424741\n"
     ]
    }
   ],
   "source": [
    "deu = pd.read_csv('deu_sentences.tsv', sep='\\t', header=None)\n",
    "eng = pd.read_csv('eng_sentences.tsv', sep='\\t')\n",
    "est = pd.read_csv('est_sentences.tsv', sep='\\t')\n",
    "fra = pd.read_csv('fra_sentences.tsv', sep='\\t')\n",
    "ita = pd.read_csv('ita_sentences.tsv', sep='\\t')\n",
    "lit = pd.read_csv('lit_sentences.tsv', sep='\\t')\n",
    "lvs = pd.read_csv('lvs_sentences.tsv', sep='\\t')\n",
    "pol = pd.read_csv('pol_sentences.tsv', sep='\\t')\n",
    "rus = pd.read_csv('rus_sentences.tsv', sep='\\t')\n",
    "spa = pd.read_csv('spa_sentences.tsv', sep='\\t')\n",
    "\n",
    "kalbos = {'deu':deu, 'eng':eng, 'est':est, 'fra':fra, 'ita':ita, 'lit':lit, 'lvs':lvs, 'pol':pol, 'rus':rus, 'spa':spa}\n",
    "\n",
    "print('Kiekvienos kalbos sakinių kiekis:')\n",
    "for k in kalbos:\n",
    "    print(f\"{k}: {len(kalbos[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13ec61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_kalbos = {}\n",
    "for k, df in kalbos.items():\n",
    "    sample_kalbos[k] = df.sample(n=6000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34b186",
   "metadata": {},
   "source": [
    "Sujungiam duomenis į vieną lentelę:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19b3e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                    sentence\n",
      "0   deu        Wievielmal die Woche essen Sie Reis?\n",
      "1   deu  Bäume verursachen die Illusion des Windes.\n",
      "2   deu    Ich weiß nicht, was ich noch sagen soll.\n",
      "3   deu                 Was ist dein Lieblingswort?\n",
      "4   deu       Er hat am Monatsende immer kein Geld.\n"
     ]
    }
   ],
   "source": [
    "for k, df in sample_kalbos.items():\n",
    "    df.columns = [\"id\", \"label\", \"sentence\"]\n",
    "    sample_kalbos[k] = df\n",
    "\n",
    "all_sen = pd.concat(sample_kalbos.values(), ignore_index=True)\n",
    "all_sen = all_sen.drop([\"id\"], axis=1)\n",
    "print(all_sen.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c11c4",
   "metadata": {},
   "source": [
    "Duomenų valymas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df4ab1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                    sentence\n",
      "0   deu        wievielmal die woche essen sie reis?\n",
      "1   deu  bäume verursachen die illusion des windes.\n",
      "2   deu    ich weiß nicht, was ich noch sagen soll.\n",
      "3   deu                 was ist dein lieblingswort?\n",
      "4   deu       er hat am monatsende immer kein geld.\n"
     ]
    }
   ],
   "source": [
    "all_sen['sentence'] = all_sen['sentence'].apply(lambda x: re.sub(r'\\d', '', str(x)).lower())\n",
    "print(all_sen.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6113b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sen = all_sen.sample(frac=1, random_state=46)   ## sumaisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b47fc6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39985                               tu izposti manu dzīvi!\n",
      "45281                    on próbuje opiekować się dziećmi.\n",
      "18181    avez-vous perdu la langue ? pourquoi ne répond...\n",
      "454      die sandsteinfassade der romanischen basilika ...\n",
      "14369                                 soovin et sa eksiks.\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = all_sen[\"sentence\"]\n",
    "y = all_sen[\"label\"]\n",
    "\n",
    "X_short_train, X_short_test, y_short_train, y_short_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=12, stratify=y\n",
    ")\n",
    "\n",
    "print(X_short_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b9c1f",
   "metadata": {},
   "source": [
    "### 1.2. Ilgi tekstai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "677bac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilgis pilno failo:  235000\n"
     ]
    }
   ],
   "source": [
    "with open(\"x_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    x1 = [line.strip() for line in f]\n",
    "\n",
    "with open(\"y_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    y1 = [line.strip() for line in f]\n",
    "\n",
    "with open(\"x_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    x2 = [line.strip() for line in f]\n",
    "\n",
    "with open(\"y_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    y2 = [line.strip() for line in f]\n",
    "\n",
    "X_full = x1 + x2\n",
    "Y_full = y1 + y2\n",
    "\n",
    "long = pd.DataFrame({\"text\": X_full, \"label\": Y_full})\n",
    "\n",
    "long.to_csv(\"full_long_data.csv\", sep=\";\", index=False, encoding=\"utf-8\")\n",
    "print(\"Ilgis pilno failo: \", len(long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81c65e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "long[\"label\"] = long[\"label\"].replace({\n",
    "    \"lav\": \"lvs\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfd829",
   "metadata": {},
   "source": [
    "Duomenų tvarkymas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1cdb67ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilgis filtruoto formato:  10000\n"
     ]
    }
   ],
   "source": [
    "long = long[long['label'].isin(['lit', 'eng', 'deu', 'fra', 'spa', 'ita', 'est', 'lvs', 'rus', 'pol'])]\n",
    "print(\"Ilgis filtruoto formato: \", len(long))\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = re.sub(r'\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4}', '', text)\n",
    "    text = re.sub(r'(1\\d{3}|2\\d{3})', '', text)\n",
    "    text = re.sub(f'\\d+', '', text)\n",
    "    text = re.sub(r'\\(\\D+\\.\\)', '', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b0d651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['klement gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. . aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. zlíni linn kandis aastatel – nime gottwaldov. ukrainas harkivi oblastis kandis zmiivi linn aastatel – nime gotvald.', \"association de recherche et de sauvegarde de l'histoire de roissy-en-france , roissy-en-france  / , mémoire d'un siècle,  (voir dans la bibliographie)\", 'dorota rabczewska, artistinimega doda (sündinud . veebruaril ) on kuulus poola rokk- ja poplaulja. ta on kõige rohkem auhindu võitnud laulja poolas. doda on tuntuks laulnud sellised hitid nagu \"szansa\", \"katharsis\" ja \"nie daj się\".', 'en navidad de , poco después de que interpretó la canción en francés película papillon (toi qui regarde la mer). ella cantará la espléndida versión de gloria aleluya para una misa de medianoche celebrada por rtl.', \"la chirurgie comprenant principalement l'ablation de la tumeur, la néphrectomie élargie (comprenant le plus souvent la surrénale et les ganglions situés à proximité)\", \"dès les années , les communes voisines d'arnouville, goussainville et dans une moindre mesure louvres, desservies par la ligne ferroviaire paris-lille connurent une expansion démographique spectaculaire avec la création de nombreux lotissements de banlieue. roissy, demeurée à l'écart du chemin de fer et de l'industrialisation restait un village agricole et s'appauvrit progressivement. le nombre de commerces passa de quatre-vingt-quatre en  à seulement dix-huit en . le même phénomène se produisit dans de nombreux autres villages du pays de france non desservis par une voie ferrée.\", 'merilai, a., maria-kristiina lotman (toim) (). methis: studia humaniora estonica, . poeetika erinumber. tartu: tartu ülikooli kirjastus', \"l'ufficio progettazione, sembra ormai certo, si trasferirà a noale, mentre alla fine anno partono i lavori di ristrutturazione degli stabilimenti di via parodi , con progetto e concessione edilizia già appese sull'esterno della fabbrica. l'operazione ha causato la messa in cassa integrazione di buona parte degli impiegati e il sorgere di numerose polemiche\", 'au er avril , les services asama sont actuellement effectués par des shinkansen série e et w. de  à , ils étaient également effectués par des shinkansen série e.', \"l'alimentation industrielle convient parfaitement à la croissance du chiot et à l'adulte. l'alimentation ménagère nécessite un supplément en vitamines et en calcium.\"]\n",
      "Mokymosi aibė:  8000\n",
      "Testavimo aibė:  2000\n"
     ]
    }
   ],
   "source": [
    "x_long = long['text'].apply(clean).tolist()\n",
    "print(x_long[:10])\n",
    "y_long = long['label'].tolist()\n",
    "X_wiki_train, X_wiki_test, y_wiki_train, y_wiki_test= train_test_split(x_long, y_long, test_size = 0.2, random_state = 42)\n",
    "print(\"Mokymosi aibė: \", len(X_wiki_train))\n",
    "print(\"Testavimo aibė: \", len(X_wiki_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49f81997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT: 1000, ENG: 1000, DEU: 1000, FRA: 1000, SPA: 1000, ITA: 1000,\n",
      " EST: 1000, LVS: 1000, RUS: 1000, POL: 1000\n"
     ]
    }
   ],
   "source": [
    "lt_lang = len(long[long['label'] == 'lit'])\n",
    "eng_lang = len(long[long['label'] == 'eng']) \n",
    "deu_lang = len(long[long['label'] == 'deu'])\n",
    "fra_lang = len(long[long['label'] == 'fra'])\n",
    "spa_lang = len(long[long['label'] == 'spa'])\n",
    "ita_lang = len(long[long['label'] == 'ita'])\n",
    "est_lang = len(long[long['label'] == 'est'])\n",
    "lav_lang = len(long[long['label'] == 'lvs'])\n",
    "rus_lang = len(long[long['label'] == 'rus'])\n",
    "pol_lang = len(long[long['label'] == 'pol'])\n",
    "print(f\"LT: {lt_lang}, ENG: {eng_lang}, DEU: {deu_lang}, FRA: {fra_lang}, SPA: {spa_lang}, ITA: {ita_lang},\\n EST: {est_lang}, LVS: {lav_lang}, RUS: {rus_lang}, POL: {pol_lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8bae3",
   "metadata": {},
   "source": [
    "# 2. Mašininio mokymosi modeliai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1b9b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c63c06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nb_model():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=(2, 5),\n",
    "            lowercase=False \n",
    "        )),\n",
    "        (\"clf\", MultinomialNB())\n",
    "    ])\n",
    "\n",
    "def make_lr_model():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=(2, 5),\n",
    "            lowercase=False\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=1000\n",
    "        ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13992a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def train_and_eval(model_name, model,\n",
    "                   X_train, y_train,\n",
    "                   X_test_main, y_test_main,\n",
    "                   X_test_other, y_test_other,\n",
    "                   train_name, main_name, other_name):\n",
    "\n",
    "   \n",
    "    print(f\"{model_name} modelis, apmokytas su {train_name} duomenimis\")\n",
    "\n",
    "\n",
    "    # Apmokome modelį\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Apmokymo trukmė: {train_time:.2f} s\")\n",
    "\n",
    "    # Testavimas ant savo tipo duomenų\n",
    "    acc_main = accuracy_score(y_test_main, model.predict(X_test_main))\n",
    "    print(f\"Tikslumas testuojant ant {main_name}:  {acc_main:.4f}\")\n",
    "\n",
    "    # Testavimas ant kito tipo duomenų\n",
    "    acc_other = accuracy_score(y_test_other, model.predict(X_test_other))\n",
    "    print(f\"Tikslumas testuojant ant {other_name}: {acc_other:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"Modelis\": model_name,\n",
    "        \"Apmokyta ant\": train_name,\n",
    "        \"Apmokymo trukmė (s)\": train_time,\n",
    "        f\"Tikslumas ant {main_name}\": acc_main,\n",
    "        f\"Tikslumas ant {other_name}\": acc_other\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "08879fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Multinominis Bajeso modelis, apmokytas su trumpų sakinių duomenimis\n",
      "======================\n",
      "Apmokymo trukmė: 7.73 s\n",
      "Tikslumas testuojant ant trumpų sakinių (testavimo aibėje):  0.9958\n",
      "Tikslumas testuojant ant ilgų tekstų (testavimo aibėje): 0.9765\n",
      "\n",
      "======================\n",
      "Logistinės regresijos modelis, apmokytas su trumpų sakinių duomenimis\n",
      "======================\n",
      "Apmokymo trukmė: 123.71 s\n",
      "Tikslumas testuojant ant trumpų sakinių (testavimo aibėje):  0.9948\n",
      "Tikslumas testuojant ant ilgų tekstų (testavimo aibėje): 0.9785\n"
     ]
    }
   ],
   "source": [
    "# NB ir LR treniruoti ant SHORT\n",
    "nb_S = make_nb_model()\n",
    "lr_S = make_lr_model()\n",
    "\n",
    "train_and_eval(\"Multinominis Bajeso\", nb_S,\n",
    "               X_short_train, y_short_train,\n",
    "               X_short_test, y_short_test,\n",
    "               X_wiki_test,  y_wiki_test,\n",
    "               train_name=\"trumpų sakinių\",\n",
    "               main_name=\"trumpų sakinių (testavimo aibėje)\",\n",
    "               other_name=\"ilgų tekstų (testavimo aibėje)\")\n",
    "\n",
    "train_and_eval(\"Logistinės regresijos\", lr_S,\n",
    "               X_short_train, y_short_train,\n",
    "               X_short_test, y_short_test,\n",
    "               X_wiki_test,  y_wiki_test,\n",
    "               train_name=\"trumpų sakinių\",\n",
    "               main_name=\"trumpų sakinių (testavimo aibėje)\",\n",
    "               other_name=\"ilgų tekstų (testavimo aibėje)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2bef5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Multinominis Bajeso modelis, apmokytas su ilgų tekstų duomenimis\n",
      "======================\n",
      "Apmokymo trukmė: 14.59 s\n",
      "Tikslumas testuojant ant ilgų tekstų (testavimo aibėje):  0.9825\n",
      "Tikslumas testuojant ant trumpų tekstų (testavimo aibėje): 0.9800\n",
      "\n",
      "======================\n",
      "Logistinės regresijos modelis, apmokytas su ilgų tekstų duomenimis\n",
      "======================\n",
      "Apmokymo trukmė: 126.97 s\n",
      "Tikslumas testuojant ant ilgų tekstų (testavimo aibėje):  0.9845\n",
      "Tikslumas testuojant ant trumpų tekstų (testavimo aibėje): 0.8876\n"
     ]
    }
   ],
   "source": [
    "# NB ir LR treniruoti ant WIKI\n",
    "nb_W = make_nb_model()\n",
    "lr_W = make_lr_model()\n",
    "\n",
    "train_and_eval(\"Multinominis Bajeso\", nb_W,\n",
    "               X_wiki_train, y_wiki_train,\n",
    "               X_wiki_test,  y_wiki_test,\n",
    "               X_short_test, y_short_test,\n",
    "               train_name=\"ilgų tekstų\", main_name=\"ilgų tekstų (testavimo aibėje)\", other_name=\"trumpų tekstų (testavimo aibėje)\")\n",
    "\n",
    "train_and_eval(\"Logistinės regresijos\", lr_W,\n",
    "               X_wiki_train, y_wiki_train,\n",
    "               X_wiki_test,  y_wiki_test,\n",
    "               X_short_test, y_short_test,\n",
    "               train_name=\"ilgų tekstų\", main_name=\"ilgų tekstų (testavimo aibėje)\", other_name=\"trumpų tekstų (testavimo aibėje)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51e869",
   "metadata": {},
   "source": [
    "# 3. Bibliotekų taikymas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "252c1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "import time\n",
    "from langdetect import detect, DetectorFactory\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bcb1fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelis jau yra: lid.176.bin\n"
     ]
    }
   ],
   "source": [
    "MODEL_URL = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "MODEL_PATH = \"lid.176.bin\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Siunčiam fastText modelį (~126 MB)…\")\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "    print(\"Baigta.\")\n",
    "else:\n",
    "    print(\"Modelis jau yra:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "396d5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_lib_to_ours = {\n",
    "    \"lt\": \"lit\",\n",
    "    \"en\": \"eng\",\n",
    "    \"de\": \"deu\",\n",
    "    \"fr\": \"fra\",\n",
    "    \"es\": \"spa\",\n",
    "    \"pl\": \"pol\",\n",
    "    \"ru\": \"rus\",\n",
    "    \"it\": \"ita\",\n",
    "    \"lv\": \"lvs\",   \n",
    "    \"et\": \"est\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad611f4d",
   "metadata": {},
   "source": [
    "Mūsų modelis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "46400018",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_short = make_nb_model()\n",
    "nb_long  = make_nb_model()\n",
    "\n",
    "nb_short.fit(X_short_train, y_short_train)\n",
    "\n",
    "nb_long.fit(X_wiki_train, y_wiki_train)\n",
    "\n",
    "def nb_short_predict(text):\n",
    "    return nb_short.predict(text)\n",
    "\n",
    "def nb_long_predict(text):\n",
    "    return nb_long.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b3c87",
   "metadata": {},
   "source": [
    "langdetect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db0010cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorFactory.seed = 0  \n",
    "\n",
    "def langdetect_predict(text: str) -> str:\n",
    "    try:\n",
    "        code = detect(text)  # pvz. \"lt\", \"en\", \"lv\", \"et\"\n",
    "    except:\n",
    "        code = \"unk\"\n",
    "    return mapping_lib_to_ours.get(code, \"unk\")\n",
    "\n",
    "def langdetect_predict_batch(texts):\n",
    "    return [langdetect_predict(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a73a17",
   "metadata": {},
   "source": [
    "fasttext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6e90dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "def fasttext_predict(text: str) -> str:\n",
    "    labels, probs = ft_model.predict(text, k=1)\n",
    "    raw_label = labels[0]                       # pvz. \"__label__en\"\n",
    "    code = raw_label.replace(\"__label__\", \"\")   # \"en\"\n",
    "    return mapping_lib_to_ours.get(code, \"unk\")\n",
    "\n",
    "def fasttext_predict_batch(texts):\n",
    "    return [fasttext_predict(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64310d6b",
   "metadata": {},
   "source": [
    "Eksperimentai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def eval_system(model_name, variant_name,\n",
    "                X_test, y_test,\n",
    "                predict_fn,\n",
    "                test_set_name):\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = predict_fn(X_test)   \n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} ({variant_name}) – {test_set_name}\")\n",
    "    print(f\"Bendras tikslumas: {acc:.4f}\")\n",
    "    print(f\"Prognozavimo laikas: {elapsed:.2f} s ({len(X_test)} tekstų)\")\n",
    "\n",
    "    results.append({\n",
    "        \"Modelis\": model_name,\n",
    "        \"Variantas\": variant_name,\n",
    "        \"Testavimo rinkinys\": test_set_name,\n",
    "        \"Bendras tikslumas\": acc,\n",
    "        \"Prognozavimo laikas (s)\": elapsed\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96201ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mūsų NB (mokytas ant trumpų sakinių) – trumpų sakinių testavimo aibė\n",
      "  Bendras tikslumas: 0.9958\n",
      "  Prognozavimo laikas: 1.47 s (12000 tekstų)\n",
      "\n",
      "Mūsų NB (mokytas ant ilgų tekstų) – ilgų tekstų testavimo aibė\n",
      "  Bendras tikslumas: 0.9825\n",
      "  Prognozavimo laikas: 2.66 s (2000 tekstų)\n",
      "\n",
      "langdetect (bibliotekos modelis) – trumpų sakinių testavimo aibė\n",
      "  Bendras tikslumas: 0.9159\n",
      "  Prognozavimo laikas: 69.70 s (12000 tekstų)\n",
      "\n",
      "langdetect (bibliotekos modelis) – ilgų tekstų testavimo aibė\n",
      "  Bendras tikslumas: 0.9760\n",
      "  Prognozavimo laikas: 10.13 s (2000 tekstų)\n",
      "\n",
      "fastText (lid.176 modelis) – trumpų sakinių testavimo aibė\n",
      "  Bendras tikslumas: 0.9505\n",
      "  Prognozavimo laikas: 0.18 s (12000 tekstų)\n",
      "\n",
      "fastText (lid.176 modelis) – ilgų tekstų testavimo aibė\n",
      "  Bendras tikslumas: 0.9800\n",
      "  Prognozavimo laikas: 0.17 s (2000 tekstų)\n",
      "\n",
      "=== REZULTATŲ LENTELĖ ===\n",
      "      Modelis                   Variantas             Testavimo rinkinys  \\\n",
      "0     Mūsų NB  mokytas ant trumpų sakinių  trumpų sakinių testavimo aibė   \n",
      "1     Mūsų NB     mokytas ant ilgų tekstų     ilgų tekstų testavimo aibė   \n",
      "2  langdetect         bibliotekos modelis  trumpų sakinių testavimo aibė   \n",
      "3  langdetect         bibliotekos modelis     ilgų tekstų testavimo aibė   \n",
      "4    fastText             lid.176 modelis  trumpų sakinių testavimo aibė   \n",
      "5    fastText             lid.176 modelis     ilgų tekstų testavimo aibė   \n",
      "\n",
      "   Bendras tikslumas  Prognozavimo laikas (s)  \n",
      "0           0.995750                 1.474998  \n",
      "1           0.982500                 2.656017  \n",
      "2           0.915917                69.702870  \n",
      "3           0.976000                10.132975  \n",
      "4           0.950500                 0.178977  \n",
      "5           0.980000                 0.166999  \n"
     ]
    }
   ],
   "source": [
    "eval_system(\n",
    "    model_name=\"Mūsų NB\",\n",
    "    variant_name=\"mokytas ant trumpų sakinių\",\n",
    "    X_test=X_short_test,\n",
    "    y_test=y_short_test,\n",
    "    predict_fn=nb_short_predict,\n",
    "    test_set_name=\"trumpų sakinių testavimo aibė\"\n",
    ")\n",
    "eval_system(\n",
    "    model_name=\"Mūsų NB\",\n",
    "    variant_name=\"mokytas ant ilgų tekstų\",\n",
    "    X_test=X_wiki_test,\n",
    "    y_test=y_wiki_test,\n",
    "    predict_fn=nb_long_predict,\n",
    "    test_set_name=\"ilgų tekstų testavimo aibė\"\n",
    ")\n",
    "eval_system(\n",
    "    model_name=\"langdetect\",\n",
    "    variant_name=\"bibliotekos modelis\",\n",
    "    X_test=X_short_test,\n",
    "    y_test=y_short_test,\n",
    "    predict_fn=langdetect_predict_batch,\n",
    "    test_set_name=\"trumpų sakinių testavimo aibė\"\n",
    ")\n",
    "\n",
    "eval_system(\n",
    "    model_name=\"langdetect\",\n",
    "    variant_name=\"bibliotekos modelis\",\n",
    "    X_test=X_wiki_test,\n",
    "    y_test=y_wiki_test,\n",
    "    predict_fn=langdetect_predict_batch,\n",
    "    test_set_name=\"ilgų tekstų testavimo aibė\"\n",
    ")\n",
    "\n",
    "eval_system(\n",
    "    model_name=\"fastText\",\n",
    "    variant_name=\"lid.176 modelis\",\n",
    "    X_test=X_short_test,\n",
    "    y_test=y_short_test,\n",
    "    predict_fn=fasttext_predict_batch,\n",
    "    test_set_name=\"trumpų sakinių testavimo aibė\"\n",
    ")\n",
    "\n",
    "eval_system(\n",
    "    model_name=\"fastText\",\n",
    "    variant_name=\"lid.176 modelis\",\n",
    "    X_test=X_wiki_test,\n",
    "    y_test=y_wiki_test,\n",
    "    predict_fn=fasttext_predict_batch,\n",
    "    test_set_name=\"ilgų tekstų testavimo aibė\"\n",
    ")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Rezultatų lentelė\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0a5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_vu] *",
   "language": "python",
   "name": "conda-env-nlp_vu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
